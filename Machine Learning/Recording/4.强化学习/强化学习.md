# **强化学习**

问题引入：如自动飞行的直升机，如何训练以使它保持平衡？

如果使用监督学习，则需要有训练集，训练集应为在何种情况下下该做什么事，如转速为x时，要倾斜多少度？等等，这是一个不确定的状态，是一个十分模糊的情况，或者说得到x的理想动作y的数据集时非常困难的。
为此，我们要使用一种新的机器学习领域——强化学习。强化学习的一个关键输入时一种叫做奖励或奖励函数的东西，它会告诉直升机什么时候做得好，什么时候做的不好。

> 符号定义：
>
> s : (states)状态
> a：(action)行动
> R(s) ：(rewards) 奖励
> r : 折扣因子
> return ：回报
> policy : 决策（策略）

符号解释：状态用于描述此时此刻的状态数据，行动表示在当下的状态进行下一步的决定，奖励用于判断行动是好还是坏，折扣因子表示在每一次做出行动时，都会导致消耗一些成本，需要将奖励进行折扣，回报表示在做完所有的行动后所得到的奖励总和，策略用于制定一系列的行动。

这种强化学习应用程序又被形式化的称为”马尔可夫决策过程“（MDP）。在马尔可夫决策过程中，未来只取决于你现在在哪里，而不是关于你怎么到这里的。

## 贝尔曼方程——也被称作动态规划方程

定义状态（动作价值函数）：
Q（s,a) :表示此时的状态s开始并且执行一次操作a，在执行一次操作a之后，您之后的行为将达到最佳状态，$Q(s,a) = R_1+rR_2+,\cdots$

贝尔曼方程：$Q(s,a) = R(s) + r\max\limits_a{Q(s',a')}$  

解释：当前状态的价值 = 行动前状态的价值 + 权重 ×(下一个行动能得到价值的最大值（选择能得到最大价值的行动a）)	

由于我们需要的是最大化折扣奖励总和的平均值，因此将贝尔曼优化可得：$Q(s,a) = R(s) + rE[\max\limits_a{Q(s',a')}]$ 其中E表示数学期望（平均值）。

## 学习状态价值函数

我们使用探月器来进行举例：其中有四个动作——nothing(什么也不做)、left(向左移动)、main(主推动器)、right(向右移动)。

1. 首先，我们将采用我们的神经网络并随机初始化神经网络中的所有参数，此时我们不知道它是否是Q(s,a)
2. 重复以下操作：
   1. 采取随机漂浮行动，这可能是好的也可能是坏的。Get(s,a,R(s),s)
   2. 存储随机得到的数据。通常情况下存储最近得到10000个类似元组，以减少内存压力，这种存储在强化学习算法中称为”重放缓冲区“
   3. 训练神经网络。使用保存最近的10000个元组进行性训练 ，x = （s，a）, $y = Q(s,a)=R(s)+rE[\max\limits_a{Q(s',a')}]$  训练出新的神经网络$Q_{new}$ .
   4. 令$Q = Q_{new}$ 
3. 分别求出Q(s,nothing) ,Q(s,left),Q(s,main),Q(s,right) 看得出这4个Q函数中，哪个值最大就执行哪个操作。



#### 算法改进1.

改进神经网络架构，使之直接输出这4个状态。

![image-20231112172202378](C:\Users\CooKu\AppData\Roaming\Typora\typora-user-images\image-20231112172202378.png)

这样可以提高效率，一次即可计算出这4个值。

#### 算法改进2.

$\varepsilon$ - 贪婪策略：

1. 以0.95的概率选择最大化Q(s,a)的动作
2. 以0.05的概率，我们随机选择一个动作

原因：假设Q(s,a)随机初始化，有一些奇怪的原因，导致算法认为启动主推动器不是一个好的主意，所以Q(s,main)总是很低，如果是这种情况，那么神经网络因为它试图选择最大化Q(s,a) 的动作a，所以它永远不会尝试启动主推动器，所以他永远不会知道启动主推动器实际上有时是一个好主意。

我们称步骤2为探索步骤，称步骤1为贪婪行动，对于$\varepsilon$ 的取值，通常以一个非常高的值进行选取（甚至取到1.0），然后开始迭代，每一次都降低$\varepsilon$ .

#### 算法改进3

小批量和软更新

1. 小批量：如果训练的数据有很多，多到几亿甚至几十亿，我们可以将这一数据集进行拆分为许多小数据集，如每个数据集设有1000个样本，然后分别对这些小数据集进行训练，最终也会收敛，这是一种更快更高效的方法。
   

   ![image-20231112172935669](C:\Users\CooKu\AppData\Roaming\Typora\typora-user-images\image-20231112172935669.png)

2. 软更新：如在设置$Q = Q_{new}$ 时，有时会将Q设置为更糟糕的$Q_{new}$ 我们将使用软更新来调整。
   

   ![image-20231112171427083](C:\Users\CooKu\AppData\Roaming\Typora\typora-user-images\image-20231112171427083.png)

   可以认为每次都将参数更新一点点

事实证明，使用软更新方法可以使强化学习算法更加可靠地收敛。

