# **逻辑回归**



## 1.sigmoid函数

$$g(z) = \frac{1}{1+e^{-z}}$$     $0<g<1$ 

sigmoid函数的特征：g取值在0 ~ 1范围内，当自变量(z)趋近于+∞时，$e^{-z}$ 趋近于0，$1 - e^{-z}$ 趋近于1，函数值g趋近于1 ；当自变量（z）趋近于-∞时，$e^{-z}$ 趋近于+∞，$1 - e^{-z}$ 趋近于-∞，函数值g趋近于0。

**函数图像：**

![image-20231104190439770](C:\Users\CooKu\AppData\Roaming\Typora\typora-user-images\image-20231104190439770.png)

针对于sigmoid函数的特殊性，我们可以依据此函数来对二分类问题进行求解：

对线性回归进行改造，令sigmoid函数的自变量z = y = ax+b ，对此可以进行二分类，如预测一个人的性别时，有大量的特征和一个对应的标签——头发的长度，眼睛的大小，身高，体重等以及一个标签“男（或女）”。 使用这写数据进行训练函数，最后可以得到一个较好的模型，如：当头发长度大于某个值且身高小于某个值的时候，输出男，否则输出女。

因为预测出的是一个数值，且时连续的，因此需要设定一个中间值，对于sigmoid函数，我们通常设定0.5为一个阈值，当预测出的值大于0.5的时候我们就输出一个类别，当预测值小于0.5的时候我们就输出另一个类别。

#### 1）代价函数

我们可以这样定义代价函数$L(f_{\vec{w},b}(\vec{x^{(i)}}),y^{(i)})$：

当y = 1 时， $L(f_{\vec{w},b}(\vec{x^{(i)}}),y^{(i)}) = - log(f_{\vec{w},b}(\vec{x^{(i)}}))$ 取对数后，当预测出的y = 1时，损失L为0 。

当y = 0 时 ，$L(f_{\vec{w},b}(\vec{x^{(i)}}),y^{(i)}) = - log(1 - f_{\vec{w},b}(\vec{x^{(i)}}))$  取对数后，当预测出的y = 0时，损失L为0 。



对代价函数进行化简可以得到：$L(f_{\vec{w},b}(\vec{x^{(i)}}),y^{(i)}) = -y^{(i)} l- log(f_{\vec{w},b}(\vec{x^{(i)}}))- (1 - y^{(i)})log(1 - f_{\vec{w},b}(\vec{x^{(i)}}))$

因为y的值只能取0或1，当y = 1时，$L(f_{\vec{w},b}(\vec{x^{(i)}}),y^{(i)}) = - log(f_{\vec{w},b}(\vec{x^{(i)}}))$ 当y = 0时： $L(f_{\vec{w},b}(\vec{x^{(i)}}),y^{(i)}) = - log(1 - f_{\vec{w},b}(\vec{x^{(i)}}))$ 正好与之对应。

对所有训练样本进行取平均误差，$J(\vec{w},b) =- \frac{1}{2m}  \sum\limits _{i = 1}^{m} [y^{(i)} log(f_{\vec{w},b}(\vec{x^{(i)}}))+ (1 - y^{(i)})log(1 - f_{\vec{w},b}(\vec{x^{(i)}}))]$

因为损失函数J还是在线性回归的基础上进行的变换，其本质上还是一个二次函数，仍然可以使用梯度下降的方法进行拟合函数。

> **另一种理解代价函数的方式：**
>
> g(x) 表示x为1的概率；1 - g(x) 表示x为0的概率；可以合并为： $P=(1 - g(x))^{1-y} + (g(x))^y$  要使该P的值尽可能大(因为当y为0时，使模型预测出y = 0的值尽可能大，当y为1使，使模型预测出1的值尽可能大)，取对数得：$log(P)=(1 - y)(1 - g(x)) + y(g(x))$ 要使$log(P)$ 的值最大，即要求$- log(P)$ 的最小值，转换为求$ -(1 - y)(1 - g(x))  - y(g(x))$的最小值，求和可得$J = \frac{1}{2m} \sum\limits_{i = 1}^{m} -(1 - y)(1 - g(x))  - y(g(x))$

#### 2）梯度下降

$w_j = w_j - \alpha \frac{\partial}{\partial{w_j}}J(w_j,b) $

$b = b - \alpha \frac{\partial}{\partial{b}}J(w_j,b)$

其中：
$\frac{\partial}{\partial{w_j}}J(w_j,b) = \frac{1}{m} \sum\limits_{i = 1}^{m}(f_{\vec{w},b}(\vec {x}^{(i)}) - y^{(i)})x_j^{(i)}$  
$\frac{\partial}{\partial{b}}J(w_j,b) = \frac{1}{m} \sum\limits_{i = 1}^{m}(f_{\vec{w},b}(\vec {x}^{(i)}) - y^{(i)})$  



#### 3）过拟合问题与欠拟合问题

如果模型不能很好的对数据进行拟合，则称具有高偏差（欠拟合）：

模型具有高方差会导致模型对训练数据拟合的不是很好，对未知的数据进行预测得到的结果也不是很好。

![image-20231104200025413](C:\Users\CooKu\AppData\Roaming\Typora\typora-user-images\image-20231104200025413.png)

如果模型对数据具有一个很好的拟合效果，则称具有很好的泛化：

模型具有很好的泛化能力，所得到的模型对训练数据具有相对较好的能力，同时也对未知数据预测具有相对较好的能力。

![image-20231104200338806](C:\Users\CooKu\AppData\Roaming\Typora\typora-user-images\image-20231104200338806.png)

如果模型对数据进行了过度的拟合，以至于对未知的数据不能拥有很好的预测效果，则称具有高方差（过拟合）:

这样会导致模型对训练数据拟合的很好以至于对训练数据进行预测的误差为0，但是对未知的数据进行预测会变得非常差。

![image-20231104200435245](C:\Users\CooKu\AppData\Roaming\Typora\typora-user-images\image-20231104200435245.png)



解决过拟合的三种方法：

1. Collect more data  ： 收集更多数据
2. Select features   ： 选择和使用特征的一个子集
3. Reduce size of parameters   ： 使用正则化来减小参数的大小

#### 4）正则化

调整参数w，对参数w进行正则化,在损失函数的后面添加一个正则项：$\frac{\lambda}{2m} \sum\limits_{i = 1}^{n} w_j^2$

$J(\vec{w},b) =- \frac{1}{2m}  \sum\limits _{i = 1}^{m} [y^{(i)} log(f_{\vec{w},b}(\vec{x^{(i)}}))+ (1 - y^{(i)})log(1 - f_{\vec{w},b}(\vec{x^{(i)}}))] + \frac{\lambda}{2m} \sum\limits_{i = 1}^{n} w_j^2$

 其中$\lambda$是一个人为设定的参数，当$\lambda$趋近于0时，可能会出现过度拟合的现象，当$\lambda$足够大时，则$J(\vec{w},b) =- \frac{1}{2m}  \sum\limits _{i = 1}^{m} [y^{(i)} log(f_{\vec{w},b}(\vec{x^{(i)}}))+ (1 - y^{(i)})log(1 - f_{\vec{w},b}(\vec{x^{(i)}}))] $ 会趋近于0，则 f (x) 趋近于b，会出现欠拟合现象。
约定只对w进行正则化而不对b进行正则化，正则化的目的是规范化w，使幂数更高的系数对函数整体影响更小。



##### 对线性回归使用正则化

梯度下降：

$w_j = w_j - \alpha[\frac{1}{m} \sum\limits_{i = 1}^{m}[(f_{\vec{w},b}(\vec{x}^{(i)} - y^{(i)})x_j^{(i)})]+\frac{\lambda}{m}w_j]$

$b = b - \alpha[\frac{1}{m} \sum\limits_{i = 1}^{m}[(f_{\vec{w},b}(\vec{x}^{(i)} - y^{(i)})]$

##### 对逻辑回归使用正则化

$w_j = w_j - \alpha [\frac{1}{m}\sum\limits_{i = 1}^{m}[(f_{\vec{w},b}(\vec{x}^{(i)} - y^{(i)})x_j^{(i)}]+\frac{\lambda}{m}w_j] $

$b = b - \alpha [\frac{1}{m}\sum\limits_{i = 1}^{m}[(f_{\vec{w},b}(\vec{x}^{(i)} - y^{(i)})x_j^{(i)}]$

