# **线性回归**

## 一、理论部分

**线性回归**：在有限的数据中，通过调整参数，来拟合出一条直线，并以此为直线（模型）来进行**预测**。

形如：$y = w\cdot x +b$ 是直线的一般形式，其中整个直线的状态由$w和b$决定，w决定直线的斜率（也就是倾斜角度），b决定直线在y轴上的截距（控制直线的上下平移）。因此，我们只需要计算出$w和b$即可确定出具体的直线，接下来研究如何找到$w和b$。

![image-20231104170457772](C:\Users\CooKu\AppData\Roaming\Typora\typora-user-images\image-20231104170457772.png)



**必要的名词：特征/输入特征，训练示例，训练集。**

| 名词 | 释义 |
| :----: |: ---- : |
| 训练集 | 用于训练模型的数据集 |
| 测试集 | 用于检测训练出来的模型是否合格 |
| 训练示例 | 用于训练模型的数据集中的每一项 |
| 特征/输入特征 | 样本中的某一类，称为某一特征 |

### 1.单变量线性回归

#### 1）数据预处理

我们先研究单变量线性回归，所谓单变量线性回归是指只有一个变量的直线，如：$y = w\cdot x +b$即为单变量直线，只具有一个输入变量x。该直线可以表示在二维平面（横轴为x，纵轴为y）。

**在拿到一组样本时，我们通常将数据分为训练集与测试集，如取样本的前80%作为训练集，剩余20%作为测试集。**(后面还会学到交叉验证集)

#### 2）定义代价函数

假定我们已经求出来了w和b，那么我们就确定了一条直线，就可以使用该直线进行预测了，为了方便判断我们所预测出来的值**y'**与真实的值**y**之间的**误差**是多少，我们要定义“一把尺子”，用于衡量预测值y'与真实值y之间的误差。因此我们使用**均方误差**来定义**代价函数**：

$$J(w,b) = \frac{1}{2m}\sum\limits_{i = 1}^m(f_{w,b}(x^{(i)}) - y^{(i)})^2$$

​	$f_{w,b}(x^{(i)}) - y^{(i)}$：其中$f_{w,b}(x^{(i)})$代表了使用我们的模型进行预测得到的值，而$y^{(i)}$代表了每一个训练示例的真实结果，这两者的差值表示了预测结果与真实结果之间的误差。（带平方的原因：在所有样本集中，每一个误差可能为正数也可能为负数，进行求和后可能会抵消，这样做会导致在每一项的误差都很大时（如：-100，+90，-25，+30），进行求和之后得到一个小的值（-5），会导致得出错误的判断。
​	$\frac{1}{2m}$:代表了对所有项的误差进行取平均值（因为我们想要知到该模型对整个样本的误差），得到均方误差。（除以2的原因是为了后面进行求导方便）。

**有了平方误差成本函数，我么只管的知到，如果该误差值越小，说明我们预测的值与真实的值越接近，即我们训练的模型越好，因此我们需要将误差成本进行减小。**

通过观察误差成本函数，可以看到误差成本函数是一个二次函数，即是一个凸函数，其极值点就是最值点，由于误差成本函数是一个开口向上的二次函数（因为当误差为0时，误差成本函数有最小值0，其他的值只可能比零大），具有最小值，即我们只需要找到一个极小值就可以得到最小值。对于误差成本函数$J(w,b)$ , 其公式展开可以写为：$$J(w,b) = \frac{1}{2m}\sum\limits_{i = 1}^m((wx^{(i)}+b) - y^{(i)})^2$$  ，J的大小取决于参数w和b，这可以通过梯度下降的方法进行求解。

![image-20231104183308038](C:\Users\CooKu\AppData\Roaming\Typora\typora-user-images\image-20231104183308038.png)

#### 3）梯度下降

梯度下降的思想，主要运行了求偏导的方法，这与生物学上的控制变量的方法很相似，如：在控制b不变的情况下去更新w（可以视为b为常数），公式：$w' =  w - \alpha \frac{\partial J(w)}{\partial w}$ 代表依次更新了w，其中$\alpha$ 代表了**学习率** 用于表示距离，$\frac{\partial J(w)}{\partial w}$ 表示对w求偏导，得到的是 W - J 凸函数上的一个切线，用于表示函数值下降最快的方向，这两者的乘积表示向函数值下降最快的方向进行移动一段距离。这个距离需要依据数据集进行调整，如果$\alpha$过大，会导致w直接越过最低点到另一端的高点，以至于无法接近最小值，如果$\alpha$过小，会导致w接近最小值时越来越慢，消耗计算成本。

![image-20231104183948198](C:\Users\CooKu\AppData\Roaming\Typora\typora-user-images\image-20231104183948198.png)

学习率的调整方法：

1. 先设置一个较小的$\alpha$ 如：0.001 。

2. 然后每次增加10倍，最大为1。

3. 确定某一值后，如：0.01。

4. 再进行3倍的处理，如：$0.01 \times 3 = 0.03 , 0.03 \times 3 = 0.09$ （这样做的目的是使收敛的速度更快）


> **求解偏导数的过程：**
>
> 求$\frac{\partial J(w)}{\partial w}$ :
>
> $\frac{\partial J(w)}{\partial w} $=$ \frac{\partial}{\partial w}\frac{1}{2m}\sum\limits_{i = 1}^{m}(f_{w,b}(x^{(i)}) - y^{(i)})^2$
>
> ​		= $\frac{\partial}{\partial w}\frac{1}{2m}\sum\limits_{i = 1}^{m}w x^{(i)} - y^{(i)})^2$
>
> ​		=$ \frac{1}{2m}\sum\limits_{i = 1}^{m}(f_{w,b}(x^{(i)}) - y^{(i)})2x^{(i)} $
>
> ​		=$ \frac{1}{m}\sum\limits_{i = 1}^{m}(f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} $
>
> 求$\frac{\partial J(w)}{\partial b}$ :
>
> $\frac{\partial J(w)}{\partial b} $=$ \frac{\partial}{\partial b}\frac{1}{2m}\sum\limits_{i = 1}^{m}(f_{w,b}(x^{(i)}) - y^{(i)})^2$
>
> ​		= $\frac{\partial}{\partial b}\frac{1}{2m}\sum\limits_{i = 1}^{m}w x^{(i)} - y^{(i)})^2$
>
> ​		=$ \frac{1}{2m}\sum\limits_{i = 1}^{m}(f_{w,b}(x^{(i)}) - y^{(i)})2 $
>
> ​		=$ \frac{1}{m}\sum\limits_{i = 1}^{m}(f_{w,b}(x^{(i)}) - y^{(i)}) $

**通过循环迭代的方法找到具体的w值与b值：**

$tmp\_w = w - \alpha \frac{\partial}{\partial w}J(w,b)$

$tmp\_b = b - \alpha \frac{\partial}{\partial b}J(w,b)$

$w = tmp_b$

$b = tmp\_b$  

**初始的时候我们随机的对w和b进行取值，然后进行迭代，可以设定当误差小于某个阈值的时候退出梯度下降，或者自定义迭代的次数，通过有限步骤可以得到一个较为理想的w值和b值。**



### 2.多变量线性回归

多变量线性回归，将维度扩充到了三维以至对维状态，如$y = w_1 x_1 + w_2 x_2 + b$ 可以理解为x轴和y轴分别为$x_1和x_2$ z轴为y，这样就是一个三维的立体状态，每一个训练示例都是三维空间的一个点，要找到一个直线来拟合该三维空间的点。

**做法：分别对每一个变量（$w_1,w_2,\dots,w_n,b$）进行梯度下降处理**

**要点：**对于多变量线性回归，其中不同的特征值取值范围不同，如：年龄特征取值范围：0~100，占地面积：$0m^2 $~$ 10000m^2$ ，还可能会有奇异样本，奇异样本数据的存在会引起训练时间增大，同时也可能导致无法收敛，因此，当存在奇异样本数据时，在进行训练之前需要对预处理数据进行归一化；反之，不存在奇异样本数据时，则可以不进行归一化。针对此问题，我们首先对数据进行**特征进行缩放**。

#### 1）数据预处理 

**数据归一化处理**

实现**数据归一化**的方法有三种方法：

1. **同除以最大值：**

   对每个特征中的所有值都除以该特征的最大值。

2. **均值归一化：**

   对每个特征中的值减去该特征的均值，然后再除以（该特征的最大值 - 该特征的最小值）

3. **Z-score标准化：**

   计算每个特征的标准差和均值

   对每个特征的所有值减去该特征的平均值，然后再除以该特征的均值
   

如果不进行归一化，那么由于特征向量中不同特征的取值相差较大，会导致目标函数变“扁”。这样在进行梯度下降的时候，梯度的方向就会偏离最小值的方向，走很多弯路，即训练时间过长。

如果进行归一化以后，目标函数会呈现比较“圆”，这样训练速度大大加快，少走很多弯路。

**数据归一化的好处：**

1）归一化后**加快了梯度下降求最优解的速度**，也即加快训练网络的收敛性；

2）归一化有可能提高精度

#### 2）代价函数（与单变量线性回归相同）

$$J(w,b) = \frac{1}{2m}\sum\limits_{i = 1}^m(f_{w,b}(x^{(i)}) - y^{(i)})^2$$

#### 3）梯度下降

   > 对于多元线性回归的梯度下降：
   >
   > $w_1 = W_1 - \alpha \frac{1}{m}\sum\limits_{i = 1}^{m}(f_{\vec{w},b}\vec{x}^{(i)} - y^{(i)})x_1^{(i)}$
   >
   > ​	$\vdots$
   >
   > $w_n = W_n - \alpha \frac{1}{m}\sum\limits_{i = 1}^{m}(f_{\vec{w},b}\vec{x}^{(i)} - y^{(i)})x_n^{(i)}$
   >
   > $b = b - \alpha \frac{1}{m}\sum\limits_{i = 1}^{m}(f_{\vec{w},b}\vec{x}^{(i)} - y^{(i)})$
   >
   > **解释：$w_1\cdots w_n$ 表示每一个变量的系数，b表示线性函数的常数项。**



### 3.正规方程

#### 1）数据预处理 

#### 2）代价函数

> 数学推导：
>
> $J = \frac{1}{2m}\sum\limits_{i = 1}^{m}(\vec{\theta}_i \vec{x}_i - y_i)^2$
>
> ​	=$\frac{1}{2m}||\vec{\theta} \vec{x} - y||^2$
>
> ​	=$\frac{1}{2m}(\vec{\theta} \vec{x} - y)^T(\vec{\theta} \vec{x} - y)$
>
> ​	=$\frac{1}{2m}(\vec{\theta}^T \vec{x}^T - y^T)(\vec{\theta} \vec{x} - y)$
>
> ​	=$\frac{1}{2m}(\vec{\theta}^T \vec{x}^T\vec{x}\vec{\theta} - y^T\vec{x}\vec{\theta} -\vec{\theta}^T\vec{x}^Ty +y^Ty )$ 

#### 3）梯度下降

对$\theta$求偏导得： $\Delta =\frac{\partial J}{\partial \theta}= \frac{1}{2m}(\frac{\partial \vec{\theta}^T \vec{x}^T\vec{x}\vec{\theta}}{\partial \theta} - \frac{\partial y^T\vec{x}\vec{\theta}}{\partial \theta} - \frac{\partial \vec{\theta}^T\vec{x}^Ty}{\partial \theta} + \frac{y^Ty}{\partial \theta})$

> 矩阵求导法则：
>
> 1. $\frac{\partial \theta^{T}A\theta}{\partial \theta} = (A + A^T)\theta$
> 2. $\frac{\partial X^{T}A}{\partial X} = A$
> 3. $\frac{\partial AX}{\partial X} =  A^T$
> 4. $\frac{\partial A}{\partial X} = 0$

可得 $\Delta = \frac{1}{2m}(\frac{\partial \vec{\theta}^T \vec{x}^T\vec{x}\vec{\theta}}{\partial \theta} - \frac{\partial y^T\vec{x}\vec{\theta}}{\partial \theta} - \frac{\partial \vec{\theta}^T\vec{x}^Ty}{\partial \theta} + \frac{y^Ty}{\partial \theta}) = \frac{1}{2m}\cdot (2x^Tx\theta - 2x^Ty) = \frac{1}{m} \cdot (x^Tx\theta - x^Ty)$
当$\Delta = 0$ 时: $x^Tx \theta = x^Ty$ ,$\theta = (x^Tx)^{-1}x^Ty$ 可以计算出 $\theta $ 的值。

**梯度下降与正规方程的比较**：

梯度下降：需要选择学习率α，需要多次迭代，当特征数量n大时也能较好适用，适用于各种类型的模型

正规方程：不需要选择学习率α，一次计算得出，需要计算 $(x^Tx)^{-1}$ ，如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为 𝑂(𝑛3) ，通常来说当 𝑛 小于10000 时还是可以接受的，只适用于线性模型，不适合逻辑回归模型等其他模型。

### 4.多项式回归

由于在某些情况下，直线难以拟合全部的数据，需要曲线来对数据进行拟合，如二次模型、三次模型等等。

一般情况下，数据的回归函数是未知的，即使已知也难以用一个简单的函数变换转化为线性模型，所以常用的做法是**多项式回归**(Polynomial Regression)，即使用多项式函数拟合数据。

**如何选择多项式的次数**：
多项式函数有多种，一般来说，需要先观察数据的形状，再去决定选用什么形式的多项式函数来处理问题。比如，从数据的散点图观察，如果有一个“弯”，就可以考虑用二次多项式（即对特征进行平方）；有两个“弯”，可以考虑用三次多项式（对特征取三次方）；有三个“弯”，则考虑用四次多项式（对特征取四次方），以此类推。
虽然真实的回归函数不一定是某个次数的多项式，但只要拟合的好，用适当的多项式来近似模拟真实的回归函数是可行的。



## 三、总结：

机器学习的一般步骤：

1. 导入数据，对数据进行性观察。
2. 选择合适的机器学习算法。
3. 对数据进行**预处理**（归一化、数据的划分）。
4. 定义**代价函数**。
5. 定义**梯度下降**函数。
6. 对算法进行**训练**，得出训练后的模型。
7. 进行模型的**评估**，调整算法参数。
8. 对数据进行**预测**。

