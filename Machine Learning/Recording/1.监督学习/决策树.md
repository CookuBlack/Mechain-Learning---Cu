# **决策树**

学习过程：
第一个关键决定：如何选择在每个节点上使用哪些特征进行拆分？

- 最大化纯度。

第二个关键决定：决定何时停止分裂？

- 当节点都进行了100%的分类
- 定义数的最大深度
- 优先级分数改进
- 如果一个节点的示例数量低于某个阈值也可以决定停止分裂

#### 纯度

熵函数：

$P_0 = 1 - P_1,~~~P_1 代表一种分类的数据占总的比例$

$H(P_1) = -p_1\log{P_1}-P_0\log{P_0} =-P_0\log{P_1} - (1 - P_1)\log{(1- P_1)}$

 ![image-20231111194026552](C:\Users\CooKu\AppData\Roaming\Typora\typora-user-images\image-20231111194026552.png)

信息增益：在决策树中，熵的减少称为信息增益。

> 定义：
>
> $p_1^{left}$ ：代表分类后左边的分类中，正确的占左边分类的比例
>
> $w^{left}$ ：代表分类后左边的分类占分类前样本的比例
>
> $p_1^{right}$：代表分类后右边的分类中，正确的占右边分类的比例
>
> $w^{right}$ ：代表分类后右边的分类占分类前样本的比例

可以定义信息增益（Information） = $H(P_1^{root}) - (W^{left}H(P_1^{left} + w^{right}H(P_1^{right})))$

### 构建决策树的全过程

- 从根节点处的所有训练示例开始，计算所有可能特征的信息增益，并选择要拆分的特征，从而提供最高的信息增益。
- 选择此功能后，您将根据所选功能将数据集拆分为两个子集，并创建树的左右分支，并将训练示例发送到左侧或右侧分支。
- 继续在树的左分支、树的右分支上重复拆分过程。
- 继续这样做，直到满足条件为止。
- 结束条件可以是：
- 1. 当一个节点100%是单一类型，熵已经达到0.
  2. 当进一步拆分节点将导致树超过您设置的最大深度。
  3. 如果信息从额外的拆分小于阈值。
  4. 节点中的示例数量低于阈值

## One-hot（独热编码）

对于决策树，如果从根节点分出了3个子节点我们该如何对其进行编码（因为此时我们不能使用0，1了）？

我们可以使用One-hot来进行编码，如果一个分类特征可以取k个可能的值，那么我们将通过创建k个只能取0或1的二进制特征来替换它。

如有三个特征：可以用（1，0，0），（0，1，0），（0，0，1）这三个编码来进行分类。

One-hot编码还可以用于神经网络中。

## 回归树（连续）

通过动物的一系列特征来预测动物的体重（体重是加权重的平均值，因为最后分类后得到的是一个方差很小的不同的值，我们只取这些连续值的平均值）。

如何选取特征？
如果选择的特征能使分类后加权方差减小，那么选取这些能使加权方差减小的最大的一个特征进行划分，从信息增益的角度说，就是选取的特征能使原方差与分类后的加权方差的差值最大。

如：在没有分类之前体重的方差为20.51，经过耳朵特征分类后，得到左边的方差为1.47，右边的方差为21.87，并且$w^{left} = \frac{5}{10},w^{right} = \frac{5}{10}$ 计算类似的信息增益：$H= 20.51 - (\frac{5}{10} 1.47 + \frac{5}{10}21.87) $ 并通过计算其他特征的分类后的信息增益，选取信息增益最大的一个特征进行划分。

## 构建多个决策树

如果将训练集中的数据换了一个或几个，将会导致拆分的最高信息增益特征变为了其他特征。对此，我们可以构建多个决策树，依次对示例进行预测，然后使用投票机制，选取投票更高那个作为预测的结果。这样做会提高决策树的健壮性。

有放回抽样：
有放回抽样可以让你构建多个不同的训练集，它与你原来的训练示例有点相似，但也有很大的不同。

## 随机森林

对已有的样本数据（设有n个）进行随机放回抽样，得到k个样本的数据集，利用该数据集进行训练1个决策上述，并再次随机放回抽样得到k个样本数据集。再训练一个决策树，继续重复，直至训练B个决策树，这B个决策树组成了随机森林（B不大于100），如果特征（n）特别大，通常的做法是令$k = \sqrt{n}$

随机森林更稳健的原因是：替换过程导致算法探索数据的许多微小变化，并且它正在训练不同的决策树，并对所有这些树进行**平均**更改程序导致的数据变化。

## XGBoost

高效的算法，其中关键的一步是在构建随机森林时，刻意使用之前预测错误的样本进行训练决策树。

使用XGBoost的方法：

```python
#分类
from xgboost import XGBClassifier
model = XGBClassifier()
model.fit(X_train,y_train)
y_pred = model.predict(x_test)
```

```python
#回归
from xgboost import XGBRegressor
model = XGBRegressor()
model.fit(X_train,y_train)
y_pred = model.predict(X_test)
```

## 何时使用决策树

决策树和随机森林通常适用于表格数据，也称为结构化数据。

决策树和随机森林不适用于非结构数据（如：图像，声音）。

神经网络：适用在非结构数据中。

神经网络与决策树和随机森林的对比

|                  | 训练速度               | 可解释性 |
| ---------------- | ---------------------- | -------- |
| 决策树或随机森林 | 快                     | 高       |
| 神经网络         | 慢（可以使用迁移学习） | 低       |

