# **神经网络**

神经网络是深度学习领域一个十分重要的概念，它将普通的算法融合在一起，通过分层的形式，构建出一个网络，并称该网络为**神经网络**。

神经网络的结构是分层的，每一层具有若干个单元，具体结构如下图：

![image-20231105103141390](C:\Users\CooKu\AppData\Roaming\Typora\typora-user-images\image-20231105103141390.png)

神经网络被分为三层结构，分别是：输入层，隐藏层，输出层。

#### 1.输入层：

顾名思义，输入层是数据进行输入的地方，将样本数据输入到输入层后，对数据进行编码调整以共后面的层进行使用。

#### 2.隐藏层

隐藏层可以有若干个层，每一层可以有不同的单元，是对数据进进一步处理，通常是一些可以调整的参数，通过训练数据进行训练得出。

#### 3.输出层

输出层只有一个层，用于输出的最后的结果，其神经单元与输出的结果密切相关，如果是分类，则输出的是一个类别，如果是回归，输出的是一个数。

神经网络的前向传播：由输入层到输出层这一过程称为“神经网络的前向传播”。

## 激活函数

通常选择的激活函数有三种：

1. sigmoid激活函数
2. Linear 激活函数（线性激活函数）
3. Relu 激活函数

其图像如下：

![image-20231111165511046](C:\Users\CooKu\AppData\Roaming\Typora\typora-user-images\image-20231111165511046.png)

如何选择激活函数：

1. 对于输出层：
   1. 如果是一个二元分类问题，使用sigmoid函数
   2. 如果是线性的，y可以取正或负，使用线性激活函数
   3. 如果y只能取正值或零值，使用relu函数
2. 对于隐藏层：只建议使用Relu激活函数（原因：Relu的计算效率更高，步骤少，Relu只有一处是平的，更方便梯度下降）

为什么模型需要激活函数:

​	加入激活函数是为了使神经网络能够模拟非线性函数，提高模型的表达能力。如果没有激活函数，神经网络的输出将是输入的线性组合，无法处理复杂的非线性问题。

​	问题1： 如果所有节点都使用线性激活函数会发生什么？
​	答：这会导致神经网络与普通的线性回归一样
​	原因：因为线性函数的线性函数本身就是线性函数

​	问题2：如果对所有隐藏层使用线性激活函数会发生什么？
​	答：这回导致神经网络与逻辑回归相同。
​	原因：逻辑回归$g(z) = \frac{1}{1 + e^{-z}},而 z = \vec{w}\vec{a} + b$ 在隐藏层种全为线性激活函数，所以到输出层之前全为线性函数，这与逻辑回归相同。

建议：中间层使用Relu，输出层在选择合适的激活函数。



模型：tensorflow



## 神经网络的softmax输出

对于计算机而言，计算的步骤越多，所得的结果误差越大，因为计算机无法使用分数来表示数值，如果时一个无限小数 ，则计算机只会保存有限个数位。

神经网络中的softmax 代码实现

```python
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
model = Sequential([
    Dense(units = 25,activation = 'relu')
    Dense(units = 15,activation = 'relu')
    Dense(units = 10,activation = 'Linear')
])
from tensorflow.keras.loss import SparseCategoricalCrossentropy
model.compile(...,loss = SparseCategoricalCrossentropy(from_logits = True))
model.fit(X,Y,epochs = 100)
logis = model(X)
f_x = tf.nn.softmax(logits)
```

高级优化方法（Adam）：
	该方法本质上是改变$\alpha（步长）$ 的大小，如果步长过小，就稍微调大，如果步长过大，就稍微调小。

核心算法：Adam ：Adaptive Moment estimation

Adam 并没有使用单一的全局学习率$\alpha$ 如：$w_1,w_2,w_3,\cdots,w_10,b$ 对于每一个参数，都会设置不同的学习率，即设置11个不同的$\alpha$ 

算法直觉：如果参数$w_j或b$似乎继续沿着大致相同的方向移动，就增加该参数的学习率，如果参数不断来回震荡，就稍微减小该参数的$\alpha _j$ 。

代码实现：

```python
model = Sequential([
    Dense(units = 25,activation = 'relu')
    Dense(units = 15,activation = 'relu')
    Dense(units = 10,activation = 'Linear')
])
model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = le - 3),
              loss = SparseCategoricalCrossentropy(from_logits = True))
model.fit(X,Y,epochs = 100)
```





## 反向传播

计算神经网络输出a的前向传播步骤：

![image-20231111180305601](C:\Users\CooKu\AppData\Roaming\Typora\typora-user-images\image-20231111180305601.png)

反向传播其实上是逆行的进行一步一步的求导——链式求导法则

![image-20231111181229179](C:\Users\CooKu\AppData\Roaming\Typora\typora-user-images\image-20231111181229179.png)



